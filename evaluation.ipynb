{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECONTRAIL Detection - Model Evaluation\n",
    "\n",
    "This notebook provides tools for evaluating contrail detection model performance.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have installed the package:\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Import ECONTRAIL detection package\n",
    "from econtrail_detection import predict_contrails\n",
    "from econtrail_detection.utils import (\n",
    "    load_image,\n",
    "    preprocess_image,\n",
    "    calculate_metrics,\n",
    "    save_prediction\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Test Images and Ground Truth\n",
    "\n",
    "Load test images and their corresponding ground truth masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "test_images_dir = Path('test_images')\n",
    "data_dir = Path('data')\n",
    "\n",
    "# List available test images\n",
    "test_images = list(test_images_dir.glob('*.png')) + list(test_images_dir.glob('*.jpg'))\n",
    "print(f\"Found {len(test_images)} test images\")\n",
    "\n",
    "# Display first test image if available\n",
    "if test_images:\n",
    "    sample_image = load_image(test_images[0])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(sample_image)\n",
    "    plt.title(f\"Sample Test Image: {test_images[0].name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No test images found. Add images to the 'test_images' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Predictions\n",
    "\n",
    "Run the contrail detection model on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_PATH = None  # Use default model, or specify path to custom model\n",
    "THRESHOLD = 0.5\n",
    "DEVICE = 'cpu'  # or 'cuda' if GPU is available\n",
    "\n",
    "# Run predictions on test images\n",
    "predictions = []\n",
    "\n",
    "if test_images:\n",
    "    print(f\"Running predictions on {len(test_images)} images...\")\n",
    "    \n",
    "    for img_path in test_images:\n",
    "        pred = predict_contrails(\n",
    "            str(img_path),\n",
    "            model_path=MODEL_PATH,\n",
    "            threshold=THRESHOLD,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        predictions.append(pred)\n",
    "        print(f\"  Processed: {img_path.name}\")\n",
    "    \n",
    "    print(\"Predictions complete!\")\n",
    "else:\n",
    "    print(\"No test images available for prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Predictions\n",
    "\n",
    "Visualize the predictions alongside original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "if test_images and predictions:\n",
    "    n_samples = min(3, len(test_images))  # Show up to 3 samples\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(12, 4 * n_samples))\n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Original image\n",
    "        image = load_image(test_images[i])\n",
    "        axes[i, 0].imshow(image)\n",
    "        axes[i, 0].set_title(f\"Original: {test_images[i].name}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[i, 1].imshow(predictions[i], cmap='jet', alpha=0.7)\n",
    "        axes[i, 1].set_title(\"Contrail Prediction\")\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No predictions to visualize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Performance (if ground truth available)\n",
    "\n",
    "Calculate metrics if ground truth masks are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth masks (if available)\n",
    "ground_truth_dir = data_dir / 'ground_truth'\n",
    "\n",
    "if ground_truth_dir.exists():\n",
    "    gt_masks = list(ground_truth_dir.glob('*.png')) + list(ground_truth_dir.glob('*.jpg'))\n",
    "    \n",
    "    if gt_masks and len(gt_masks) == len(predictions):\n",
    "        print(\"Calculating metrics...\\n\")\n",
    "        \n",
    "        all_metrics = []\n",
    "        for i, (pred, gt_path) in enumerate(zip(predictions, gt_masks)):\n",
    "            gt = load_image(gt_path)\n",
    "            if len(gt.shape) == 3:\n",
    "                gt = gt[:, :, 0]  # Use first channel if multi-channel\n",
    "            \n",
    "            metrics = calculate_metrics(pred, gt)\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            print(f\"Image {i+1}: {test_images[i].name}\")\n",
    "            print(f\"  Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  Recall:    {metrics['recall']:.3f}\")\n",
    "            print(f\"  F1-Score:  {metrics['f1_score']:.3f}\")\n",
    "            print(f\"  IoU:       {metrics['iou']:.3f}\")\n",
    "            print()\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {\n",
    "            key: np.mean([m[key] for m in all_metrics])\n",
    "            for key in all_metrics[0].keys()\n",
    "        }\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"AVERAGE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        for key, value in avg_metrics.items():\n",
    "            print(f\"{key.capitalize():12s}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"Ground truth available but count mismatch: {len(gt_masks)} GT vs {len(predictions)} predictions\")\n",
    "else:\n",
    "    print(f\"No ground truth directory found at: {ground_truth_dir}\")\n",
    "    print(\"Create 'data/ground_truth' directory and add ground truth masks to evaluate performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Predictions\n",
    "\n",
    "Save prediction masks to disk for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('output/predictions')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save predictions\n",
    "if predictions:\n",
    "    print(f\"Saving predictions to {output_dir}...\")\n",
    "    \n",
    "    for i, (pred, img_path) in enumerate(zip(predictions, test_images)):\n",
    "        output_path = output_dir / f\"pred_{img_path.stem}.png\"\n",
    "        save_prediction(pred, output_path, colormap=True)\n",
    "        print(f\"  Saved: {output_path.name}\")\n",
    "    \n",
    "    print(\"All predictions saved!\")\n",
    "else:\n",
    "    print(\"No predictions to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading test images\n",
    "2. Running contrail detection predictions\n",
    "3. Visualizing results\n",
    "4. Evaluating performance (with ground truth)\n",
    "5. Saving predictions\n",
    "\n",
    "For more information, see the [research paper](https://doi.org/10.1109/TGRS.2025.3629628)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
