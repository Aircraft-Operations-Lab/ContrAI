{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECONTRAIL Detection - Dataset Loading and Testing\n",
    "\n",
    "This notebook demonstrates how to load and test the modified dataset.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Load and explore the modified dataset\n",
    "- Verify data integrity\n",
    "- Visualize samples\n",
    "- Test data preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Import ECONTRAIL detection utilities\n",
    "from econtrail_detection.utils import (\n",
    "    load_image,\n",
    "    preprocess_image\n",
    ")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "Explore the structure and contents of the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "data_dir = Path('data')\n",
    "\n",
    "# Check if dataset exists\n",
    "if not data_dir.exists():\n",
    "    print(f\"Dataset directory not found: {data_dir}\")\n",
    "    print(\"Creating directory structure...\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (data_dir / 'images').mkdir(exist_ok=True)\n",
    "    (data_dir / 'masks').mkdir(exist_ok=True)\n",
    "    (data_dir / 'ground_truth').mkdir(exist_ok=True)\n",
    "    print(\"Directory structure created!\")\n",
    "else:\n",
    "    print(f\"Dataset directory found: {data_dir}\")\n",
    "\n",
    "# List subdirectories\n",
    "subdirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
    "print(f\"\\nSubdirectories: {[d.name for d in subdirs]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count files in each subdirectory\n",
    "for subdir in subdirs:\n",
    "    image_files = list(subdir.glob('*.png')) + list(subdir.glob('*.jpg')) + list(subdir.glob('*.tif'))\n",
    "    print(f\"{subdir.name:20s}: {len(image_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset Samples\n",
    "\n",
    "Load and display sample images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from 'images' subdirectory\n",
    "images_dir = data_dir / 'images'\n",
    "\n",
    "if images_dir.exists():\n",
    "    image_files = list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.tif'))\n",
    "    \n",
    "    if image_files:\n",
    "        print(f\"Found {len(image_files)} images in dataset\")\n",
    "        \n",
    "        # Display first few samples\n",
    "        n_samples = min(6, len(image_files))\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            img = load_image(image_files[i])\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"{image_files[i].name}\\nShape: {img.shape}\")\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(n_samples, 6):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No images found in 'data/images' directory.\")\n",
    "        print(\"Add your dataset images to this directory.\")\n",
    "else:\n",
    "    print(f\"Images directory not found: {images_dir}\")\n",
    "    print(\"Create 'data/images' directory and add your dataset images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Image-Mask Pairs\n",
    "\n",
    "Check if images have corresponding masks and visualize them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for corresponding masks\n",
    "masks_dir = data_dir / 'masks'\n",
    "\n",
    "if images_dir.exists() and masks_dir.exists():\n",
    "    image_files = list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg'))\n",
    "    \n",
    "    if image_files:\n",
    "        print(\"Checking image-mask pairs...\\n\")\n",
    "        \n",
    "        paired_data = []\n",
    "        for img_path in image_files:\n",
    "            # Look for corresponding mask\n",
    "            mask_path = masks_dir / img_path.name\n",
    "            if not mask_path.exists():\n",
    "                # Try with different extension\n",
    "                mask_path = masks_dir / f\"{img_path.stem}.png\"\n",
    "            \n",
    "            if mask_path.exists():\n",
    "                paired_data.append((img_path, mask_path))\n",
    "        \n",
    "        print(f\"Found {len(paired_data)} paired image-mask samples\")\n",
    "        print(f\"Missing masks: {len(image_files) - len(paired_data)}\")\n",
    "        \n",
    "        # Visualize paired samples\n",
    "        if paired_data:\n",
    "            n_samples = min(3, len(paired_data))\n",
    "            fig, axes = plt.subplots(n_samples, 2, figsize=(12, 4 * n_samples))\n",
    "            if n_samples == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            for i in range(n_samples):\n",
    "                img_path, mask_path = paired_data[i]\n",
    "                \n",
    "                # Load and display image\n",
    "                img = load_image(img_path)\n",
    "                axes[i, 0].imshow(img)\n",
    "                axes[i, 0].set_title(f\"Image: {img_path.name}\")\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                # Load and display mask\n",
    "                mask = load_image(mask_path)\n",
    "                if len(mask.shape) == 3:\n",
    "                    mask = mask[:, :, 0]  # Use first channel\n",
    "                axes[i, 1].imshow(mask, cmap='gray')\n",
    "                axes[i, 1].set_title(f\"Mask: {mask_path.name}\")\n",
    "                axes[i, 1].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No images found to check for pairs.\")\n",
    "else:\n",
    "    print(\"Images or masks directory not found.\")\n",
    "    print(\"Create 'data/images' and 'data/masks' directories with your data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Data Preprocessing\n",
    "\n",
    "Test the preprocessing pipeline on sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing with different configurations\n",
    "if images_dir.exists():\n",
    "    image_files = list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg'))\n",
    "    \n",
    "    if image_files:\n",
    "        # Load a sample image\n",
    "        sample_img = load_image(image_files[0])\n",
    "        print(f\"Original image shape: {sample_img.shape}\")\n",
    "        print(f\"Original data type: {sample_img.dtype}\")\n",
    "        print(f\"Original value range: [{sample_img.min()}, {sample_img.max()}]\")\n",
    "        \n",
    "        # Test different preprocessing options\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "        \n",
    "        # Original\n",
    "        axes[0, 0].imshow(sample_img)\n",
    "        axes[0, 0].set_title(f\"Original\\nShape: {sample_img.shape}\")\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Normalized\n",
    "        normalized = preprocess_image(sample_img, normalize=True)\n",
    "        axes[0, 1].imshow(normalized)\n",
    "        axes[0, 1].set_title(f\"Normalized\\nRange: [{normalized.min():.2f}, {normalized.max():.2f}]\")\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # Resized\n",
    "        resized = preprocess_image(sample_img, target_size=(256, 256))\n",
    "        axes[1, 0].imshow(resized)\n",
    "        axes[1, 0].set_title(f\"Resized\\nShape: {resized.shape}\")\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Normalized + Resized\n",
    "        both = preprocess_image(sample_img, target_size=(256, 256), normalize=True)\n",
    "        axes[1, 1].imshow(both)\n",
    "        axes[1, 1].set_title(f\"Normalized + Resized\\nShape: {both.shape}\")\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nPreprocessing test complete!\")\n",
    "    else:\n",
    "        print(\"No images available for preprocessing test.\")\n",
    "else:\n",
    "    print(\"Images directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Statistics\n",
    "\n",
    "Calculate and display dataset statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dataset statistics\n",
    "if images_dir.exists():\n",
    "    image_files = list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg'))\n",
    "    \n",
    "    if image_files:\n",
    "        print(\"Calculating dataset statistics...\\n\")\n",
    "        \n",
    "        shapes = []\n",
    "        sizes = []\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            img = load_image(img_path)\n",
    "            shapes.append(img.shape)\n",
    "            sizes.append(img_path.stat().st_size / 1024)  # Size in KB\n",
    "        \n",
    "        # Display statistics\n",
    "        print(f\"Total images: {len(image_files)}\")\n",
    "        print(f\"\\nImage shapes (unique): {set(shapes)}\")\n",
    "        print(f\"\\nFile sizes:\")\n",
    "        print(f\"  Min:  {min(sizes):.2f} KB\")\n",
    "        print(f\"  Max:  {max(sizes):.2f} KB\")\n",
    "        print(f\"  Mean: {np.mean(sizes):.2f} KB\")\n",
    "        \n",
    "        # Plot file size distribution\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(sizes, bins=20, edgecolor='black')\n",
    "        plt.xlabel('File Size (KB)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of Image File Sizes')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No images found for statistics calculation.\")\n",
    "else:\n",
    "    print(\"Images directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Dataset Metadata (Optional)\n",
    "\n",
    "Generate a metadata file for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset metadata\n",
    "if images_dir.exists():\n",
    "    image_files = list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg'))\n",
    "    \n",
    "    if image_files:\n",
    "        metadata = {\n",
    "            'dataset_name': 'ECONTRAIL Modified Dataset',\n",
    "            'num_images': len(image_files),\n",
    "            'image_files': [img.name for img in image_files],\n",
    "            'created': str(Path.cwd()),\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = data_dir / 'dataset_metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        print(f\"Metadata saved to: {metadata_path}\")\n",
    "        print(f\"\\nMetadata summary:\")\n",
    "        print(f\"  Dataset: {metadata['dataset_name']}\")\n",
    "        print(f\"  Images:  {metadata['num_images']}\")\n",
    "    else:\n",
    "        print(\"No images found to create metadata.\")\n",
    "else:\n",
    "    print(\"Images directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading and exploring the modified dataset structure\n",
    "2. Visualizing dataset samples\n",
    "3. Verifying image-mask pairs\n",
    "4. Testing the preprocessing pipeline\n",
    "5. Calculating dataset statistics\n",
    "6. Creating dataset metadata\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Add more images to `data/images/`\n",
    "- Add corresponding masks to `data/masks/`\n",
    "- Use the `evaluation.ipynb` notebook to run model predictions\n",
    "- See the [research paper](https://doi.org/10.1109/TGRS.2025.3629628) for more details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
